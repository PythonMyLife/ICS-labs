#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
# Student ID : 517030910183
# Name : Hou Zhengtong
# Describe how and why you modified the baseline code.
# 1: change jxx stuctures to avoid load/use hazzard.
# 2: loop unrolling and by testing with 2,4,5,6,7,8 ,I find that 7 is the best option.
# 3: Use iaddq instruction,and this instuction will decrease load num into register
# 4: When finish loop,I unfold all the loops to finish the unfinished loops because loop will add load/use hazzard.
# 5: in the loop function,store the previous and load the behind.In this way, we can reduce the CPE using less independent load instruction.
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	xorq %rax,%rax		# count = 0;
	iaddq $-7,%rdx
	jmp Whether
L7:	
	mrmovq (%rdi), %r10	
	iaddq $56, %rdi		
	rmmovq %r10, (%rsi)	
	iaddq $56, %rsi
	andq %r10, %r10	
	mrmovq -48(%rdi), %r10	
	jle L6		
	iaddq $1, %rax
L6:		
	rmmovq %r10, -48(%rsi)
	andq %r10, %r10
	mrmovq -40(%rdi), %r10	
	jle L5		
	iaddq $1, %rax		
L5:		
	rmmovq %r10, -40(%rsi)	
	andq %r10, %r10	
	mrmovq -32(%rdi), %r10
	jle L4		
	iaddq $1, %rax		
L4:		
	rmmovq %r10, -32(%rsi)	
	andq %r10, %r10	
	mrmovq -24(%rdi), %r10
	jle L3		
	iaddq $1, %rax		
L3:		
	rmmovq %r10, -24(%rsi)	
	andq %r10, %r10
	mrmovq -16(%rdi), %r10		
	jle L2		
	iaddq $1, %rax		
L2:		
	rmmovq %r10, -16(%rsi)	
	andq %r10, %r10
	mrmovq -8(%rdi), %r10		
	jle L1		
	iaddq $1, %rax		
L1:		
	rmmovq %r10, -8(%rsi)	
	andq %r10, %r10
	jle L0		
	iaddq $1, %rax
L0:
	iaddq $-7, %rdx	
Whether:
	jge L7
	iaddq $3,%rdx
	jg P014
	jl P004
	jmp P4
P004:
	iaddq $2, %rdx
	jl P1
	jg P3
	jmp P2
P014:
	iaddq $-1, %rdx
	je P5
P6:
	mrmovq 40(%rdi), %r10
	rmmovq %r10, 40(%rsi)
	andq %r10, %r10
P5:
	mrmovq 32(%rdi), %r10
	jle P05
	iaddq $1, %rax
P05:
	rmmovq %r10, 32(%rdi)
	andq %r10, %r10
P4:
	mrmovq 24(%rdi), %r10
	jle P04
	iaddq $1, %rax
P04:
	rmmovq %r10, 24(%rsi)
	andq %r10, %r10
P3:
	mrmovq 16(%rdi), %r10
	jle P03
	iaddq $1, %rax
P03:
	rmmovq %r10, 16(%rsi)
	andq %r10, %r10
P2:
	mrmovq 8(%rdi), %r10
	jle P02
	iaddq $1, %rax
P02:
	rmmovq %r10, 8(%rsi)
	andq %r10, %r10
P1:
	mrmovq (%rdi), %r10
	jle P01
	iaddq $1, %rax
P01:
	rmmovq %r10, (%rsi)
	andq %r10, %r10
	jle P0
	iaddq $1, %rax
P0:
	irmovq $0, %rdx
##################################################################
# Do not modify the following section of code
# Function epilogue.

Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad -2
	.quad -3
	.quad 4
	.quad -5
	.quad -6
	.quad 7
	.quad -8
	.quad -9
	.quad 10
	.quad -11
	.quad 12
	.quad -13
	.quad 14
	.quad -15
	.quad 16
	.quad 17
	.quad 18
	.quad 19
	.quad -20
	.quad -21
	.quad -22
	.quad 23
	.quad -24
	.quad -25
	.quad 26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad -31
	.quad -32
	.quad -33
	.quad 34
	.quad 35
	.quad 36
	.quad 37
	.quad 38
	.quad -39
	.quad -40
	.quad 41
	.quad -42
	.quad 43
	.quad 44
	.quad 45
	.quad -46
	.quad -47
	.quad -48
	.quad 49
	.quad 50
	.quad -51
	.quad -52
	.quad 53
	.quad -54
	.quad -55
	.quad 56
	.quad 57
	.quad -58
	.quad 59
	.quad -60
	.quad -61
	.quad 62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
